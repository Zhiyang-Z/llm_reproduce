model:
  vocabulary_size: 50000
  embedding_size: 768
  training_seq_len: 128
  nlayer: [12,12]
  nhead: 12
  ndim: 768
  ndim_feedforward: 2048
  drop_out: 0.1
  pre_norm: True
